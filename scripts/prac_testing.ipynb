{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 13: Text Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# the next line provides graphs of better quality on HiDPI screens\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to use progress_apply, read more at https://pypi.org/project/tqdm/#pandas-integration\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   clean_description  price_realised\n",
      "0  \\nAffixed to a stained mahogany stand, restore...          1875.0\n",
      "1  \\n10 in. (25.5 cm.) high, 15 in. (38 cm.) wide...          1625.0\n",
      "Num Datapoints = 53903\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/model_data.pkl')\n",
    "\n",
    "df = data[['clean_description', 'price_realised']]\n",
    "print(df.head(2))\n",
    "print(f'Num Datapoints = {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing an evaluation routine which can be used for all models in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        # F1 Score\n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'F1 Score') \n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'ROC Curve')\n",
    "        \n",
    "        # PRC\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'PRC')        \n",
    "\n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    print(df_eval_stats)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume all models below accepts texts in lowercase and without any digits, punctuations marks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ty/Code/Pets/Christies/env/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['clean_description'] = df['clean_description'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the whole dataset is already divided into train/test one parts. The corresponding flag is 'ds_part'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43122,)\n",
      "(10781,)\n",
      "(43122,)\n",
      "(10781,)\n"
     ]
    }
   ],
   "source": [
    "dat = df.sample(frac=1)\n",
    "train_idx = int(len(dat) * 0.8)\n",
    "train = dat[:train_idx]\n",
    "test = dat[train_idx:]\n",
    "\n",
    "train_text = train['clean_description']\n",
    "test_text = test['clean_description']\n",
    "\n",
    "train_targ = train['price_realised']\n",
    "test_targ = test['price_realised']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(test_text.shape)\n",
    "print(train_targ.shape)\n",
    "print(test_targ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0 - Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354466.1169047399\n"
     ]
    }
   ],
   "source": [
    "dum = DummyClassifier(random_state=1)\n",
    "dum.fit(train_text, train_targ)\n",
    "dum_pred = dum.predict(test_text)\n",
    "print(metrics.mean_absolute_error(test_targ, dum_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - NLTK, TF-IDF and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43122, 47384)\n",
      "(10781, 47384)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_preprocessing(train_feature_text, test_feature_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vec = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "    train_tfidf = vec.fit_transform(train_feature_text)\n",
    "    test_tfidf = vec.transform(test_feature_text)\n",
    "    \n",
    "    return train_tfidf, test_tfidf\n",
    "\n",
    "train_tfidf, test_tfidf = tfidf_preprocessing(train_text, test_text)\n",
    "smol_tr = train_tfidf[:10000]\n",
    "smol_ts = test_tfidf[:2000]\n",
    "y1_smol = train_targ[:10000]\n",
    "y2_smol = test_targ[:2000]\n",
    "\n",
    "print(train_tfidf.shape)\n",
    "print(test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509395.5036959888\n",
      "499203.95610983414\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor().fit(smol_tr, y1_smol)\n",
    "dt_p = dt.predict(smol_ts)\n",
    "print(metrics.mean_absolute_error(y2_smol, dt_p))\n",
    "\n",
    "dt1 = DecisionTreeRegressor().fit(train_tfidf, train_targ)\n",
    "dt1_p = dt1.predict(test_tfidf)\n",
    "print(metrics.mean_absolute_error(test_targ, dt1_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450507.7198169687\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1)\n",
    "rf.fit(smol_tr, y1_smol)\n",
    "rf_pred = rf.predict(smol_ts)\n",
    "print(metrics.mean_absolute_error(y2_smol, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf, open('rf_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499667.5569045351\n"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "xg = xgb.XGBRegressor().fit(train_tfidf, train_targ)\n",
    "xgp = xg.predict(test_tfidf)\n",
    "print(metrics.mean_absolute_error(test_targ, xgp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fitted model with pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - spaCy, TF-IDF and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \\ncandy bars from candy counter in the store \\...\n",
       "1                                                         \n",
       "2                                                         \n",
       "3        \\ncane allo specchio (dog in the mirror) \\npla...\n",
       "4        \\nan american ship in hong kong harbor \\noil o...\n",
       "                               ...                        \n",
       "43117    \\n\\n2 volumes in-12 (176 x 103 mm). frontispic...\n",
       "43118    \\nolive trees \\nsigned and dated 'john aldridg...\n",
       "43119    \\nle cirque à l'étoile (m. 436) \\nlithograph i...\n",
       "43120    \\nmost 20th century, \\nthree cups, <i>mark of ...\n",
       "43121    \\n(indonesian, 1907-1990)\\nbarong\\nsigned with...\n",
       "Name: clean_description, Length: 43122, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = train_text.reset_index(drop=True)\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = list(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after spacy\n",
      "(43122,)\n",
      "(10781,)\n",
      "---\n",
      "after tfidf\n",
      "(43122, 42807)\n",
      "(10781, 42807)\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemma(series_text):\n",
    "    \n",
    "    new_series = []\n",
    "    for row in list(series_text.index):\n",
    "        doc = nlp(series_text.loc[row])\n",
    "        #tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "        text = ' '.join(tokens)\n",
    "        new_series.append(text)\n",
    "    return pd.Series(new_series)\n",
    "\n",
    "train_text_s = spacy_lemma(train_text)\n",
    "test_text_s = spacy_lemma(test_text)\n",
    "\n",
    "print('after spacy')\n",
    "print(train_text_s.shape)\n",
    "print(test_text_s.shape)\n",
    "\n",
    "train_text_ts, test_text_ts = tfidf_preprocessing(train_text_s, test_text_s)\n",
    "\n",
    "print('---')\n",
    "print('after tfidf')\n",
    "print(train_text_ts.shape)\n",
    "print(test_text_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498811.85662448563\n"
     ]
    }
   ],
   "source": [
    "xg = xgb.XGBRegressor().fit(train_text_ts, train_targ)\n",
    "xgp = xg.predict(test_text_ts)\n",
    "print(metrics.mean_absolute_error(test_targ, xgp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - spaCy, TF-IDF and LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34497, 42807)\n",
      "(8625, 42807)\n",
      "(34497,)\n",
      "(8625,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the train set for 20% validation to tune parameters max_depth & num_leaves\n",
    "\n",
    "train_ts, val_ts, ytrain, yval = train_test_split(train_text_ts, train_targ, test_size=0.2, random_state=1)\n",
    "\n",
    "print(train_ts.shape)\n",
    "print(val_ts.shape)\n",
    "print(ytrain.shape)\n",
    "print(yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=35, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=1, reg_alpha=0,\n",
       "             reg_lambda=1, ...)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = xgb.XGBRegressor(max_depth=35, random_state=1)\n",
    "lgbm.fit(train_text_ts, train_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lgbm, open('xgboost_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
